{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Familiar with FrozenLake Environment\n",
    "\n",
    " Gymnasium is a toolkit for developing and comparing reinforcement learning algorithms. It provides a wide variety of environments (including games, tasks, and simulations) that can be used to test and benchmark different algorithms.<br>\n",
    " \n",
    " In this notebook, we will explore the FrozenLake environment using the Gymnasium library. We will implement a simple Q-learning algorithm to train an agent to navigate the FrozenLake environment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Environment\n",
    "\n",
    "In Gymnasium, an environment is a class that simulates a problem to be solved. The environment provides feedback to the agent in the form of observations and rewards, based on the actions taken by the agent.\n",
    "\n",
    "### Main Functions of an Environment\n",
    "\n",
    "1. **`reset()`**: Resets the environment to an initial state and returns the initial observation.\n",
    "\n",
    "2. **`step(action)`**: Takes an action and returns the next state, reward, done flag, and additional info.\n",
    "\n",
    "3. **`render()`**: Renders the current state of the environment (optional).\n",
    "\n",
    "4. **`close()`**: Closes the environment and releases any resources.\n",
    "\n",
    "## What is FrozenLake?\n",
    "\n",
    "FrozenLake is a classic reinforcement learning environment where the agent must navigate a frozen lake to reach a goal without falling into holes. The environment is represented as a grid where each cell can be either a safe frozen surface or a dangerous hole. The agent starts in the top left corner of the grid and must find the goal at the bottom right corner. Each succesful episode leads to a reward of 1. This is an example of a sparse reward function that is commonly used in reinforcement learning. Agent is only rewarded for finding the goal and is not punished for falling into holes or taking more steps.\n",
    "\n",
    "## States and Actions\n",
    "\n",
    "- **States**: The state represents the current situation of the agent in the environment. In FrozenLake, each state corresponds to a cell in the grid.\n",
    "\n",
    "- **Actions**: Actions are the possible moves the agent can take. In FrozenLake, actions typically include moving up, down, left, or right.\n",
    "\n",
    "### Observation Space and Action Space\n",
    "\n",
    "- **Observation Space**: The set of all possible states. For FrozenLake, it includes all cells in the grid.\n",
    "\n",
    "- **Action Space**: The set of all possible actions the agent can take. For FrozenLake, this includes four actions: up, down, left, and right. The action space is discrete and has a mapping from action index to action name. For example: `0: up`, `1: down`, `2: left`, `3: right`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Q-learning in FrozenLake\n",
    "\n",
    "Now, let's implement a simple Q-learning algorithm to train an agent to navigate the FrozenLake environment.<br>\n",
    "\n",
    "Let's install some of the dependencies required to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gymnasium # Install the gymnasium package\n",
    "! pip install numpy # Install the numpy package to handle arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the necessary libraries and define the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # For generating random numbers\n",
    "import gymnasium as gym # For creating the environment\n",
    "import numpy as np # For working with arrays\n",
    "\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gym.make function is used to create the environment. It can take in  arguments like:\n",
    "\n",
    "- `is_slippery`: Whether the environment is slippery or not. For FrozenLake, it is set to `True`. This adds some stochasticity to the environment.\n",
    "\n",
    "- `render_mode`: The rendering mode. In this case, it is set to `None` which is the default. It is used to disable the rendering of the environment. Other rendering modes can be used, such as `human` or `rgb_array`.\n",
    "\n",
    "- `desc`: A 2D array representing the map of the environment. For FrozenLake, it is set to `None` which is the default. It is used to define the map of the environment.\n",
    "\n",
    "- `map_name`: The name of the map. It is used to define the size of the environment. For example: `4x4`, `8x8`, etc. By default it is set to `4x4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Learning rate\n",
    "gamma = 0.99 # Discount factor\n",
    "epsilon = 0.1 # Exploration rate\n",
    "num_episodes = 10000 # Number of episodes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above variables are used to define the hyperparameters of the Q-learning algorithm.<br>\n",
    "\n",
    "- `alpha`: The learning rate is a hyperparameter that controls the speed of the learning process. It is set to `0.1` by default.\n",
    "\n",
    "- `gamma`: The discount factor is a hyperparameter that controls the importance of future rewards. It is set to `0.99` by default.\n",
    "\n",
    "- `epsilon`: The exploration rate is a hyperparameter that controls the randomness of the agent. It is set to `0.1` by default.\n",
    "\n",
    "- `num_episodes`: The number of episodes is a hyperparameter that controls the number of episodes to run. It is set to `10000` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a greedy epsilon function.<br>\n",
    "\n",
    "- `greedy_epsilon`: This function takes in the Q-table, the state, and returns the action with the highest Q-value for the given state.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_epsilon(qtable, state):\n",
    "  if random.uniform(0, 1) < epsilon: # Choose a random action with probability epsilon (Exploring)\n",
    "    action = env.action_space.sample() \n",
    "  else: # Choose the action with the highest q-value (Exploiting)\n",
    "    if np.all(qtable[state, :]) == qtable[state, 0]: # If all q-values are 0, choose a random action\n",
    "      action = env.action_space.sample()\n",
    "    else: # Choose the action with the highest q-value\n",
    "      action = np.argmax(qtable[state, :])\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement the Q-learning algorithm by writing a function run_env.<br>\n",
    "\n",
    "- `run_env`: This function runs the Q-learning algorithm for a specified number of episodes and returns the Q-table.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env():\n",
    "  qtable = np.zeros((env.observation_space.n, env.action_space.n)) # Initialize the qtable with zeros\n",
    "  rewards = [] # List to store rewards per episode\n",
    "  steps = [] # List to store number of steps per episode\n",
    "  for _ in range(num_episodes): \n",
    "    state, _ = env.reset() # Reset the environment\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    episode_steps = 0\n",
    "\n",
    "    while not done:\n",
    "      action = greedy_epsilon(qtable, state) # Choose an action using epsilon-greedy\n",
    "      new_state, reward, terminated, truncated, info = env.step(action) # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      next_best_action = np.argmax(qtable[new_state, :]) # Find the action with the highest q-value for the next state\n",
    "      td_target = reward + gamma * qtable[new_state, next_best_action] # Calculate the temporal difference target\n",
    "      td_error = td_target - qtable[state, action] # Calculate the temporal difference error\n",
    "      qtable[state, action] += alpha * td_error # Update the qtable using the Bellman equation\n",
    "      state = new_state # Set the current state as the next state\n",
    "      episode_rewards += reward\n",
    "      episode_steps += 1\n",
    "      done = terminated or truncated # Check if the episode is done\n",
    "\n",
    "    rewards.append(episode_rewards)\n",
    "    steps.append(episode_steps)\n",
    "\n",
    "  print(\"Episodes complete\")\n",
    "  print(f\"Mean reward: {np.mean(rewards)}\")\n",
    "  print(f\"Max steps: {np.max(steps)}\")\n",
    "  return qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate the performance of the Q-learning algorithm by writing a function evaluation.<br>\n",
    "\n",
    "- `evaluation`: This function takes in the Q-table, the number of trials, and returns the mean reward, max number of steps taken, and success rate.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(qtable, num_trials): \n",
    "  num_successes = 0 # Number of successful episodes\n",
    "  rewards = [] # List to store rewards per episode\n",
    "  steps = [] # List to store number of steps per episode\n",
    "  for _ in range(num_trials):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    while not done:\n",
    "      action = np.argmax(qtable[state, :]) # Choose the action with the highest q-value\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "      state = new_state\n",
    "      episode_reward += reward\n",
    "      episode_steps += 1\n",
    "      done = terminated or truncated # Check if the episode is done\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    steps.append(episode_steps)\n",
    "    if episode_reward == 1: # Check if the episode was successful\n",
    "      num_successes += 1\n",
    "\n",
    "  print(\"Evaluation complete\")\n",
    "  print(f\"Mean reward: {np.mean(rewards)}\")\n",
    "  print(f\"Max steps: {np.max(steps)}\")\n",
    "  print(f\"Success rate: {num_successes / num_trials}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this implementation, we can now train the Q-learning algorithm and evaluate its performance. Now let's run the Q-learning algorithm and evaluate its performance.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = run_env()\n",
    "evaluation(qtable, 10)\n",
    "# Also make sure to close the environment when we are done.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The success rate of the policy while evaluating will fluctuate. This is because of the randomness of the environment because the `is_slippery` attribute is set to True. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
