{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing environment complexity on learning.\n",
    "\n",
    "We did a simple q-learning agent to understand the basics. Now we will write code that is used to benchmark the performance of the agent across different environments of different complexities.\n",
    "\n",
    "This code can be found in the Gymnasium Documentation as well. You can access it [here](https://gymnasium.farama.org/tutorials/training_agents/FrozenLake_tuto/#sphx-glr-tutorials-training-agents-frozenlake-tuto-py).\n",
    "\n",
    "We have properly gone through the code and understood before adding it to our project as it helps us to highlight the key observations we made throughout our project in the form of plots and graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "Install the required libraries.\n",
    "These include the following:\n",
    "\n",
    "- `gymnasium`: for the environment\n",
    "\n",
    "- `matplotlib`: for plotting\n",
    "\n",
    "- `numpy`: for arrays\n",
    "\n",
    "- `pandas`: for dataframes\n",
    "\n",
    "- `seaborn`: for heatmaps\n",
    "\n",
    "- `tqdm`: for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib numpy pandas seaborn tqdm gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import them into our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # to work with file paths\n",
    "from typing import NamedTuple  # for type safety and for better code documentation\n",
    "\n",
    "import matplotlib.pyplot as plt  # for plotting purposes\n",
    "import numpy as np  # to work with arrays\n",
    "import pandas as pd  # to work with DataFrames\n",
    "import seaborn as sns  # to make heatmaps\n",
    "from tqdm import tqdm  # to visualise iterables like loops in the form of a progress bar\n",
    "\n",
    "import gymnasium as gym  # reinforcement learning library\n",
    "# to make custom sized maps\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "sns.set_theme()  # setting the deffault theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a class called Params which contains all the parameters we will use in our code. This inherits from the NamedTuple class to ensure type safety. Then we initialise it with some default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(NamedTuple):\n",
    "    total_episodes: int  # Total episodes\n",
    "    learning_rate: float  # Learning rate\n",
    "    gamma: float  # Discounting rate\n",
    "    epsilon: float  # Exploration probability\n",
    "    map_size: int  # Number of tiles of one side of the squared environment\n",
    "    seed: int  # Define a seed so that we get reproducible results\n",
    "    is_slippery: bool  # If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions\n",
    "    n_runs: int  # Number of runs\n",
    "    action_size: int  # Number of possible actions\n",
    "    state_size: int  # Number of possible states\n",
    "    proba_frozen: float  # Probability that a tile is frozen\n",
    "    savefig_folder: Path  # Root folder where plots are saved\n",
    "\n",
    "\n",
    "params = Params(\n",
    "    total_episodes=1000,\n",
    "    learning_rate=0.8,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1,\n",
    "    map_size=5,\n",
    "    seed=123,\n",
    "    is_slippery=False,\n",
    "    n_runs=20,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    savefig_folder=Path(\"plots\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we intialise the random number generator using the seed. We also make a directory to save our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(params.seed)\n",
    "\n",
    "params.savefig_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a Qlearning class which we will use to train our agent. We also create an EpsilonGreedy class which we will use to explore or exploit our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning:\n",
    "    def __init__(self, learning_rate, gamma, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.reset_qtable()\n",
    "\n",
    "    def update(self, state, action, reward, new_state):\n",
    "        delta = (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.qtable[new_state, :])\n",
    "            - self.qtable[state, action]\n",
    "        )\n",
    "        q_update = self.qtable[state, action] + self.learning_rate * delta\n",
    "        return q_update\n",
    "\n",
    "    def reset_qtable(self):\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "    def adjust_reward(self, reward):\n",
    "        return reward\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, action_space, state, qtable):\n",
    "        explor_exploit_tradeoff = rng.uniform(0, 1)\n",
    "\n",
    "        if explor_exploit_tradeoff < self.epsilon:\n",
    "            action = action_space.sample()\n",
    "\n",
    "        else:\n",
    "\n",
    "            if np.all(qtable[state, :]) == qtable[state, 0]:\n",
    "                action = action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(qtable[state, :])\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class Qlearning contains the following functions:\n",
    "\n",
    "- `update`: Updates the Q-table for a given state-action pair based on the reward and the maximum Q-value of the next state.\n",
    "\n",
    "- `reset_qtable`: Resets the Q-table to zero.\n",
    "\n",
    "- `adjust_reward`: Reward shaping can be done here in order to improve the performance of the agent. But for now we are using the baseline reward structure.\n",
    "\n",
    "The class EpsilonGreedy contains the following function:\n",
    "\n",
    "- `choose_action`: Choose an action based on the exploration-exploitation tradeoff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before visualising, we can process the data collected into dataframes which are easier to use with seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(episodes, params, rewards, steps, map_size):\n",
    "    res = pd.DataFrame(\n",
    "        data={\n",
    "            \"Episodes\": np.tile(episodes, reps=params.n_runs),\n",
    "            \"Rewards\": rewards.flatten(),\n",
    "            \"Steps\": steps.flatten(),\n",
    "        }\n",
    "    )\n",
    "    res[\"cum_rewards\"] = rewards.cumsum(axis=0).flatten(order=\"F\")\n",
    "    res[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", res.shape[0])\n",
    "\n",
    "    st = pd.DataFrame(data={\"Episodes\": episodes, \"Steps\": steps.mean(axis=1)})\n",
    "    st[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", st.shape[0])\n",
    "    return res, st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the visualisation functions which we will use to visualise our agent's performance as graphs and heatmaps are created. \n",
    "First is the function to extract the directions from the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtable_directions_map(qtable, map_size):\n",
    "    qtable_val_max = qtable.max(axis=1).reshape(map_size, map_size)\n",
    "    qtable_best_action = np.argmax(qtable, axis=1).reshape(map_size, map_size)\n",
    "    directions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "    qtable_directions = np.empty(qtable_best_action.flatten().shape, dtype=str)\n",
    "    eps = np.finfo(float).eps\n",
    "    for idx, val in enumerate(qtable_best_action.flatten()):\n",
    "        if qtable_val_max.flatten()[idx] > eps:\n",
    "\n",
    "            qtable_directions[idx] = directions[val]\n",
    "    qtable_directions = qtable_directions.reshape(map_size, map_size)\n",
    "    return qtable_val_max, qtable_directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the function to create the heatmap of the Q-table and then the distribution graph for states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values_map(qtable, env, map_size):\n",
    "    qtable_val_max, qtable_directions = qtable_directions_map(qtable, map_size)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    ax[0].imshow(env.render())\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Last frame\")\n",
    "\n",
    "    sns.heatmap(\n",
    "        qtable_val_max,\n",
    "        annot=qtable_directions,\n",
    "        fmt=\"\",\n",
    "        ax=ax[1],\n",
    "        cmap=sns.color_palette(\"Blues\", as_cmap=True),\n",
    "        linewidths=0.7,\n",
    "        linecolor=\"black\",\n",
    "        xticklabels=[],\n",
    "        yticklabels=[],\n",
    "        annot_kws={\"fontsize\": \"xx-large\"},\n",
    "    ).set(title=\"Learned Q-values\\nArrows represent best action\")\n",
    "    for _, spine in ax[1].spines.items():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(0.7)\n",
    "        spine.set_color(\"black\")\n",
    "    img_title = f\"frozenlake_q_values_{map_size}x{map_size}.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_states_actions_distribution(states, actions, map_size):\n",
    "    labels = {\"LEFT\": 0, \"DOWN\": 1, \"RIGHT\": 2, \"UP\": 3}\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    sns.histplot(data=states, ax=ax[0], kde=True)\n",
    "    ax[0].set_title(\"States\")\n",
    "    sns.histplot(data=actions, ax=ax[1])\n",
    "    ax[1].set_xticks(list(labels.values()), labels=labels.keys())\n",
    "    ax[1].set_title(\"Actions\")\n",
    "    fig.tight_layout()\n",
    "    img_title = f\"frozenlake_states_actions_distrib_{map_size}x{map_size}.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a function which will plot the sum of cumulative rewards and average number of steps over episodes for each map size which gives us an idea of how well the agent is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_and_rewards(rewards_df, steps_df):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    sns.lineplot(\n",
    "        data=rewards_df, x=\"Episodes\", y=\"cum_rewards\", hue=\"map_size\", ax=ax[0]\n",
    "    )\n",
    "    ax[0].set(ylabel=\"Cumulated rewards\")\n",
    "\n",
    "    sns.lineplot(data=steps_df, x=\"Episodes\",\n",
    "                 y=\"Steps\", hue=\"map_size\", ax=ax[1])\n",
    "    ax[1].set(ylabel=\"Averaged steps number\")\n",
    "\n",
    "    for axi in ax:\n",
    "        axi.legend(title=\"map size\")\n",
    "    fig.tight_layout()\n",
    "    img_title = \"frozenlake_steps_and_rewards.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that out of the way we can write the run function which will train our agent for multiple runs to account for the randomness. The run function will then save the Q-table after each run along with episode rewards and the number of steps taken for each episode.<br>\n",
    "\n",
    "We then write the code to call all the functions defined to train our agent for different map sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env():\n",
    "    rewards = np.zeros((params.total_episodes, params.n_runs))\n",
    "    steps = np.zeros((params.total_episodes, params.n_runs))\n",
    "    episodes = np.arange(params.total_episodes)\n",
    "    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "\n",
    "    for run in range(params.n_runs):\n",
    "        learner.reset_qtable()\n",
    "\n",
    "        for episode in tqdm(\n",
    "            episodes, desc=f\"Run {run}/{params.n_runs} - Episodes\", leave=False\n",
    "        ):\n",
    "            state = env.reset(seed=params.seed)[0]\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = explorer.choose_action(\n",
    "                    action_space=env.action_space, state=state, qtable=learner.qtable\n",
    "                )\n",
    "\n",
    "                all_states.append(state)\n",
    "                all_actions.append(action)\n",
    "\n",
    "                new_state, reward, terminated, truncated, info = env.step(\n",
    "                    action)\n",
    "                done = terminated or truncated\n",
    "                adjusted_reward = learner.adjust_reward(reward)\n",
    "\n",
    "                learner.qtable[state, action] = learner.update(\n",
    "                    state, action, adjusted_reward, new_state\n",
    "                )\n",
    "\n",
    "                total_rewards += adjusted_reward\n",
    "                step += 1\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            rewards[episode, run] = total_rewards\n",
    "            steps[episode, run] = step\n",
    "        qtables[run, :, :] = learner.qtable\n",
    "\n",
    "    return rewards, steps, episodes, qtables, all_states, all_actions\n",
    "\n",
    "map_sizes = [4, 7, 9, 11]\n",
    "res_all = pd.DataFrame()\n",
    "st_all = pd.DataFrame()\n",
    "\n",
    "for map_size in map_sizes:\n",
    "    env = gym.make(\n",
    "        \"FrozenLake-v1\",\n",
    "        is_slippery=params.is_slippery,\n",
    "        render_mode=\"rgb_array\",\n",
    "        desc=generate_random_map(\n",
    "            size=map_size, p=params.proba_frozen, seed=params.seed\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    params = params._replace(action_size=env.action_space.n)\n",
    "    params = params._replace(state_size=env.observation_space.n)\n",
    "    env.action_space.seed(\n",
    "        params.seed\n",
    "    )\n",
    "    learner = Qlearning(\n",
    "        learning_rate=params.learning_rate,\n",
    "        gamma=params.gamma,\n",
    "        state_size=params.state_size,\n",
    "        action_size=params.action_size,\n",
    "    )\n",
    "    explorer = EpsilonGreedy(\n",
    "        epsilon=params.epsilon,\n",
    "    )\n",
    "\n",
    "    print(f\"Map size: {map_size}x{map_size}\")\n",
    "    rewards, steps, episodes, qtables, all_states, all_actions = run_env()\n",
    "    print(f\"Average Steps {np.mean(steps)} for map {map_size}x{map_size}\")\n",
    "    print(f\"Average Rewards {np.mean(rewards)} for map {map_size}x{map_size}\")\n",
    "\n",
    "    res, st = postprocess(episodes, params, rewards, steps, map_size)\n",
    "    res_all = pd.concat([res_all, res])\n",
    "    st_all = pd.concat([st_all, st])\n",
    "    qtable = qtables.mean(axis=0)\n",
    "\n",
    "    plot_states_actions_distribution(\n",
    "        states=all_states, actions=all_actions, map_size=map_size\n",
    "    )\n",
    "    plot_q_values_map(qtable, env, map_size)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "plot_steps_and_rewards(res_all, st_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we interpret the findings from the plots and graphs?\n",
    "The initial two plots give us an idea of agent behavior not necessarily how well it performs. The next set of images will tell us a bit about policy performance. If the last frame shows the agent at the goal position, it usually means that the policy derived during training is good. However, this is not always the case but it offers at least some idea about our training. The heatmap provides us with a visual representation of the policy generated by the agent after training. The arrows represent the best action at that state.<br>\n",
    "\n",
    "The final graph summarises the learning behavior of our agent over all the map sizes we have trained it on. The sum of cumulative rewards should increase if our agent is learning and the average number of steps should decrease if the agent is learning. This will also give us an idea of learning behaviour with environmental complexity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
